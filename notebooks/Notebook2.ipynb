{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLmgzMZnb4QU"
   },
   "source": [
    "# GEO-AI Challenge for Cropland Mapping by ITU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxokd6oMcbwN"
   },
   "source": [
    "Authenticate and initialize Earth Engine.\n",
    "\n",
    "**You will need an Earth Engine account. It can be requested having a Google account at https://earthengine.google.com/**\n",
    "\n",
    "You will be prompted to enter your credentials after running the lines below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kAU3u37XcRhr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic & Built-in\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Google Earth Engine\n",
    "import ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4mHiKwYy4zOH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "SEED = 2023\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRYOIoJIcU7h"
   },
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zJR66Gmzj9It",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the files - make sure to have the correct data path\n",
    "\n",
    "####### Please sepcify where you are saving the data - same path will be used to save any outputs ###########\n",
    "data_dr = './data'\n",
    "\n",
    "# Train data\n",
    "tr_sud = pd.read_csv(f'{data_dr}/train_sudan.csv')\n",
    "tr_irn = pd.read_csv(f'{data_dr}/train_iran.csv')\n",
    "tr_afg = pd.read_csv(f'{data_dr}/train_afg.csv')\n",
    "\n",
    "# Test data\n",
    "te_sud = pd.read_csv(f'{data_dr}/test_sudan.csv')\n",
    "te_irn = pd.read_csv(f'{data_dr}/test_iran.csv')\n",
    "te_afg = pd.read_csv(f'{data_dr}/test_afg.csv')\n",
    "\n",
    "\n",
    "# Submission file\n",
    "sample_submission = pd.read_csv(f'{data_dr}/SampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svpe_-b2dSzV",
    "outputId": "55c4a5cc-c198-4c6c-83fc-dc2ea8519f10",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=al5qMqnoAqLx6TcUxIov6q47iCQC-ZkbbnbJ24opEbI&tc=6wREB2xT-scIMJ8ksZdRDL187IU70iF9UjwZVxQsR6A&cc=-MIn756EIwm1fOlzlpUF-R9eRWVCD2lUSapNMHY9Z6Q>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=al5qMqnoAqLx6TcUxIov6q47iCQC-ZkbbnbJ24opEbI&tc=6wREB2xT-scIMJ8ksZdRDL187IU70iF9UjwZVxQsR6A&cc=-MIn756EIwm1fOlzlpUF-R9eRWVCD2lUSapNMHY9Z6Q</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter verification code:  4/1AfJohXl2EHmoui1yJmZg6xpHVCc1pJp79J0jEyNc2DLlgQ6PYdKQQjyRRkI\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# Get authetication token and sign in to Google Earth Engine\n",
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a48ieKLZef0t"
   },
   "source": [
    "## Extract the datasets from Google Earth Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW40jQW6Xj5K",
    "tags": []
   },
   "source": [
    "### Sentinel-2\n",
    "Load Sentinel-2 imagery from Earth Engine and select the bands:\n",
    "\n",
    "          [B2, B3, B4, B5 ,B6, B7, B8A, B11, B12, B8]\n",
    "\n",
    "the monthly mean value extractd fot the entire time interval, as following:\n",
    "\n",
    "1. Afghanistan: extracted the monthly mean value of April 2022. Also, monthly mean values for the past 3 months (Jan 2022 ~ March 2022) has been extracted and added to the database for a near real time assessment.\n",
    "2. Iran: July 2019 ~ June 2020.\n",
    "3. Sudan: July 2019 ~ June 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASybNls0Xj5L",
    "outputId": "50fe2793-5695-4925-a26f-f8fca8c8ae85",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-01-01 00:00:00  to  2022-01-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1573.42it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1229.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-01-31 00:00:00  to  2022-03-02 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1527.73it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1410.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-03-02 00:00:00  to  2022-04-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1237.12it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1481.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-04-01 00:00:00  to  2022-05-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1477.78it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1051.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.54 s, sys: 68.9 ms, total: 3.61 s\n",
      "Wall time: 38.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BANDS = ['B2', 'B3', 'B4','B5','B6','B7','B8A','B11','B12', 'B8']\n",
    "\n",
    "### Read the CSV table\n",
    "tr_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(tr_afg['Lon'], tr_afg['Lat'])]\n",
    "te_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(te_afg['Lon'], te_afg['Lat'])]\n",
    "\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2022-04-30\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "t = 1\n",
    "while start_date < end_date:\n",
    "    suffix = f'_t{t}'\n",
    "    BANDS_suffixes =[band + suffix for band in BANDS]\n",
    "\n",
    "    tr_afg[BANDS_suffixes] = None\n",
    "    te_afg[BANDS_suffixes] = None\n",
    "\n",
    "    current_date = start_date\n",
    "    next_date = start_date + timedelta(days=30)\n",
    "    print('############', current_date, ' to ', next_date)\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').filterDate(current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "    #Extract train data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(tr_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(tr_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        tr_afg.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "\n",
    "     #Extract test data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(te_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(te_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        te_afg.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "    start_date = next_date\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpKH_hN8Xj5M",
    "outputId": "7e20561a-58a2-49c5-c84b-c5eacbeb1cd3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 45)\n",
      "(500, 46)\n"
     ]
    }
   ],
   "source": [
    "print(te_afg.shape)\n",
    "print(tr_afg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFzhoq0tXj5N",
    "outputId": "6d28838c-3e02-42a9-d9ac-d9fc6ecb8936",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-01 00:00:00  to  2019-07-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1574.64it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1164.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-31 00:00:00  to  2019-08-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1368.41it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1526.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-08-30 00:00:00  to  2019-09-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1121.11it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1513.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-09-29 00:00:00  to  2019-10-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1435.31it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1476.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-10-29 00:00:00  to  2019-11-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1445.11it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 943.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-11-28 00:00:00  to  2019-12-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1142.18it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1446.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-12-28 00:00:00  to  2020-01-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1318.06it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1186.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-01-27 00:00:00  to  2020-02-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 782.65it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1271.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-02-26 00:00:00  to  2020-03-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1208.91it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 763.51it/s]\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-03-27 00:00:00  to  2020-04-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1332.32it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1350.29it/s]\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-04-26 00:00:00  to  2020-05-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 642.16it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1188.32it/s]\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-05-26 00:00:00  to  2020-06-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1303.90it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 648.43it/s]\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/3033152139.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-06-25 00:00:00  to  2020-07-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1299.83it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1229.36it/s]\n"
     ]
    }
   ],
   "source": [
    "BANDS = ['B2', 'B3', 'B4','B5','B6','B7','B8A','B11','B12', 'B8']\n",
    "\n",
    "# Read the points geometries from the CSV table\n",
    "tr_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(tr_sud['Lon'], tr_sud['Lat'])]\n",
    "te_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(te_sud['Lon'], te_sud['Lat'])]\n",
    "\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(\"2019-07-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2020-06-30\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "t = 1\n",
    "while start_date < end_date:\n",
    "    suffix = f'_t{t}'\n",
    "    BANDS_suffixes =[band + suffix for band in BANDS]\n",
    "\n",
    "    tr_sud[BANDS_suffixes] = None\n",
    "    te_sud[BANDS_suffixes] = None\n",
    "\n",
    "    current_date = start_date\n",
    "    next_date = start_date + timedelta(days=30)\n",
    "    print('############', current_date, ' to ', next_date)\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').filterDate(current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "    #Extract train data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(tr_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(tr_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        tr_sud.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "\n",
    "     #Extract test data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(te_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(te_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        te_sud.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "    start_date = next_date\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vgrva2xeXj5N",
    "outputId": "d0272b44-2fab-4423-93c8-c84d6e8ec364",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 136)\n",
      "(500, 135)\n"
     ]
    }
   ],
   "source": [
    "print(tr_sud.shape)\n",
    "print(te_sud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cFdQdgzXj5O",
    "outputId": "ef4ffa92-6fab-45ff-c1fd-8f9052d10b5b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-01 00:00:00  to  2019-07-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1608.17it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1551.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-31 00:00:00  to  2019-08-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1541.72it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1517.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-08-30 00:00:00  to  2019-09-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1513.79it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 638.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-09-29 00:00:00  to  2019-10-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1360.87it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1309.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-10-29 00:00:00  to  2019-11-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1330.27it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1289.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-11-28 00:00:00  to  2019-12-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1456.01it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1314.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-12-28 00:00:00  to  2020-01-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1393.59it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1413.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-01-27 00:00:00  to  2020-02-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1296.32it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1396.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-02-26 00:00:00  to  2020-03-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1377.20it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1379.68it/s]\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-03-27 00:00:00  to  2020-04-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1349.99it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1219.71it/s]\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-04-26 00:00:00  to  2020-05-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1197.77it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:01<00:00, 486.78it/s]\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-05-26 00:00:00  to  2020-06-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1214.49it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1323.15it/s]\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/1309579946.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_irn[BANDS_suffixes] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-06-25 00:00:00  to  2020-07-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1293.53it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1107.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the points geometries from the CSV table\n",
    "tr_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(tr_irn['Lon'], tr_irn['Lat'])]\n",
    "te_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(te_irn['Lon'], te_irn['Lat'])]\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(\"2019-07-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2020-06-30\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "t = 1\n",
    "while start_date < end_date:\n",
    "    suffix = f'_t{t}'\n",
    "    BANDS_suffixes =[band + suffix for band in BANDS]\n",
    "\n",
    "    tr_irn[BANDS_suffixes] = None\n",
    "    te_irn[BANDS_suffixes] = None\n",
    "\n",
    "    current_date = start_date\n",
    "    next_date = start_date + timedelta(days=30)\n",
    "    print('############', current_date, ' to ', next_date)\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED').filterDate(current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "    #Extract train data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(tr_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(tr_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        tr_irn.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "\n",
    "     #Extract test data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(te_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(te_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        te_irn.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "    start_date = next_date\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1NZ6BumXj5P",
    "outputId": "fe5c61aa-5f5b-4b27-faa2-85395e3034ab",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 136)\n",
      "(500, 135)\n"
     ]
    }
   ],
   "source": [
    "print(tr_irn.shape)\n",
    "print(te_irn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AeB83UKUXj5P",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_afg.to_csv(f'tr_afg_s2.csv', index=False )\n",
    "te_afg.to_csv(f'te_afg_s2.csv', index=False)\n",
    "\n",
    "tr_sud.to_csv(f'tr_sud_s2.csv', index=False)\n",
    "te_sud.to_csv(f'te_sud_s2.csv', index=False)\n",
    "\n",
    "tr_irn.to_csv(f'tr_irn_s2.csv', index=False)\n",
    "te_irn.to_csv(f'te_irn_s2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuOu65pXXj5P"
   },
   "source": [
    "### Soil Moisture\n",
    "Load The SMAP Level-4 (L4) Soil Moisture product from Earth Engine and select the bands:\n",
    "\n",
    "         'sm_surface', 'sm_rootzone',\n",
    "         'sm_surface_wetness','sm_rootzone_wetness',\n",
    "          'surface_temp', 'land_evapotranspiration_flux',\n",
    "         'vegetation_greenness_fraction', 'leaf_area_index'\n",
    "\n",
    "the monthly mean value extractd fot the entire time interval, as following:\n",
    "\n",
    "1. Afghanistan: extracted the monthly mean value of April 2022. Also, monthly mean values for the past 3 months (Jan 2022 ~ March 2022) has been extracted and added to the database for a near real time assessment.\n",
    "2. Iran: July 2019 ~ June 2020.\n",
    "3. Sudan: July 2019 ~ June 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KOc8AWF0Xj5Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "tr_sud = pd.read_csv(f'{data_dr}/train_sudan.csv')\n",
    "tr_irn = pd.read_csv(f'{data_dr}/train_iran.csv')\n",
    "tr_afg = pd.read_csv(f'{data_dr}/train_afg.csv')\n",
    "\n",
    "# Test data\n",
    "te_sud = pd.read_csv(f'{data_dr}/test_sudan.csv')\n",
    "te_irn = pd.read_csv(f'{data_dr}/test_iran.csv')\n",
    "te_afg = pd.read_csv(f'{data_dr}/test_afg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PLZJDOEXj5Q",
    "outputId": "84531dc0-0a5d-4fbd-c81d-ea7704f84b31",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-01-01 00:00:00  to  2022-01-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1621.40it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1613.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-01-31 00:00:00  to  2022-03-02 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1624.01it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1439.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-03-02 00:00:00  to  2022-04-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1581.90it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1572.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-04-01 00:00:00  to  2022-05-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1164.05it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1459.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.19 s, sys: 54 ms, total: 3.24 s\n",
      "Wall time: 7min 24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "BANDS = ['sm_surface', 'sm_rootzone','sm_surface_wetness','sm_rootzone_wetness',\n",
    "         'surface_temp', 'land_evapotranspiration_flux',\n",
    "         'vegetation_greenness_fraction', 'leaf_area_index' ]\n",
    "\n",
    "# Read the points geometries from the CSV table\n",
    "tr_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(tr_afg['Lon'], tr_afg['Lat'])]\n",
    "te_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(te_afg['Lon'], te_afg['Lat'])]\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2022-04-30\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "t = 1\n",
    "while start_date < end_date:\n",
    "    suffix = f'_t{t}'\n",
    "    BANDS_suffixes =[band + suffix for band in BANDS]\n",
    "\n",
    "    tr_afg[BANDS_suffixes] = None\n",
    "    te_afg[BANDS_suffixes] = None\n",
    "\n",
    "    current_date = start_date\n",
    "    next_date = start_date + timedelta(days=30)\n",
    "    print('############', current_date, ' to ', next_date)\n",
    "    collection = (ee.ImageCollection('NASA/SMAP/SPL4SMGP/007').filterDate(current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "    #Extract train data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(tr_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(tr_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        tr_afg.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "\n",
    "     #Extract test data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(te_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(te_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        te_afg.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "    start_date = next_date\n",
    "    t+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kM3Ra9jmXj5Q",
    "outputId": "e61f93dc-ebe5-4a47-e65d-59da5b7229a4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-01 00:00:00  to  2019-07-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1181.36it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1647.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-31 00:00:00  to  2019-08-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1512.74it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1174.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-08-30 00:00:00  to  2019-09-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1468.05it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1537.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-09-29 00:00:00  to  2019-10-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1578.05it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1516.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-10-29 00:00:00  to  2019-11-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1066.81it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1563.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-11-28 00:00:00  to  2019-12-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1550.02it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1524.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-12-28 00:00:00  to  2020-01-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1501.52it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 918.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-01-27 00:00:00  to  2020-02-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1462.35it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1436.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-02-26 00:00:00  to  2020-03-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 836.18it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1326.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-03-27 00:00:00  to  2020-04-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1428.80it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 750.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-04-26 00:00:00  to  2020-05-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1307.75it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1386.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-05-26 00:00:00  to  2020-06-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1375.00it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1442.20it/s]\n",
      "/tmp/ipykernel_2757002/973793766.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  tr_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n",
      "/tmp/ipykernel_2757002/973793766.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  te_sud[BANDS_suffixes] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-06-25 00:00:00  to  2020-07-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1411.71it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1314.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the points geometries from the CSV table\n",
    "tr_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(tr_sud['Lon'], tr_sud['Lat'])]\n",
    "te_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(te_sud['Lon'], te_sud['Lat'])]\n",
    "\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(\"2019-07-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2020-06-30\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "t = 1\n",
    "while start_date < end_date:\n",
    "    suffix = f'_t{t}'\n",
    "    BANDS_suffixes =[band + suffix for band in BANDS]\n",
    "\n",
    "    tr_sud[BANDS_suffixes] = None\n",
    "    te_sud[BANDS_suffixes] = None\n",
    "\n",
    "    current_date = start_date\n",
    "    next_date = start_date + timedelta(days=30)\n",
    "    print('############', current_date, ' to ', next_date)\n",
    "    collection = (ee.ImageCollection('NASA/SMAP/SPL4SMGP/007').filterDate(current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "    #Extract train data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(tr_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(tr_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        tr_sud.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "\n",
    "     #Extract test data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(te_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(te_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        te_sud.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "    start_date = next_date\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCvSHnHXXj5R",
    "outputId": "16cd9dad-938a-42a7-a14a-3e33b2a9b145",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-01 00:00:00  to  2019-07-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1389.12it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1405.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-31 00:00:00  to  2019-08-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1608.80it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1509.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-08-30 00:00:00  to  2019-09-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1448.33it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1597.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-09-29 00:00:00  to  2019-10-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1542.52it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1410.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-10-29 00:00:00  to  2019-11-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1471.04it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1448.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-11-28 00:00:00  to  2019-12-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1361.01it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 623.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-12-28 00:00:00  to  2020-01-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1489.32it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1457.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-01-27 00:00:00  to  2020-02-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1458.89it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1513.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-02-26 00:00:00  to  2020-03-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1311.51it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1455.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-03-27 00:00:00  to  2020-04-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1338.69it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 532.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-04-26 00:00:00  to  2020-05-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1397.06it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1363.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-05-26 00:00:00  to  2020-06-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1277.81it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1367.32it/s]\n",
      "<timed exec>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-06-25 00:00:00  to  2020-07-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1265.98it/s]\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 531.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 340 ms, total: 12.8 s\n",
      "Wall time: 39min 51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Read the points geometries from the CSV table\n",
    "tr_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(tr_irn['Lon'], tr_irn['Lat'])]\n",
    "te_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(te_irn['Lon'], te_irn['Lat'])]\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(\"2019-07-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2020-06-30\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "t = 1\n",
    "while start_date < end_date:\n",
    "    suffix = f'_t{t}'\n",
    "    BANDS_suffixes =[band + suffix for band in BANDS]\n",
    "\n",
    "    tr_irn[BANDS_suffixes] = None\n",
    "    te_irn[BANDS_suffixes] = None\n",
    "\n",
    "    current_date = start_date\n",
    "    next_date = start_date + timedelta(days=30)\n",
    "    print('############', current_date, ' to ', next_date)\n",
    "    collection = (ee.ImageCollection('NASA/SMAP/SPL4SMGP/007').filterDate(current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "    #Extract train data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(tr_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(tr_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        tr_irn.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "\n",
    "     #Extract test data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(te_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(te_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        te_irn.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "    start_date = next_date\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7B_nM5x-Xj5R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_afg.to_csv(f'tr_afg_smap.csv', index=False )\n",
    "te_afg.to_csv(f'te_afg_smap.csv', index=False)\n",
    "\n",
    "tr_sud.to_csv(f'tr_sud_smap.csv', index=False)\n",
    "te_sud.to_csv(f'te_sud_smap.csv', index=False)\n",
    "\n",
    "tr_irn.to_csv(f'tr_irn_smap.csv', index=False)\n",
    "te_irn.to_csv(f'te_irn_smap.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry0rR_VgXj5R"
   },
   "source": [
    "### Sentinel-1\n",
    "\n",
    "Load Sentinel-1 imagery from Earth Engine and select the bands:\n",
    "\n",
    "        [VV, VH]\n",
    "\n",
    "the monthly mean value extractd fot the entire time interval, as following:\n",
    "\n",
    "1. Afghanistan: extracted the monthly mean value of April 2022. Also, monthly mean values for the past 3 months (Jan 2022 ~ March 2022) has been extracted and added to the database for a near real time assessment.\n",
    "2. Iran: July 2019 ~ June 2020.\n",
    "3. Sudan: July 2019 ~ June 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "xfmklrzHXj5R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "tr_sud = pd.read_csv(f'{data_dr}/train_sudan.csv')\n",
    "tr_irn = pd.read_csv(f'{data_dr}/train_iran.csv')\n",
    "tr_afg = pd.read_csv(f'{data_dr}/train_afg.csv')\n",
    "\n",
    "# Test data\n",
    "te_sud = pd.read_csv(f'{data_dr}/test_sudan.csv')\n",
    "te_irn = pd.read_csv(f'{data_dr}/test_iran.csv')\n",
    "te_afg = pd.read_csv(f'{data_dr}/test_afg.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5elZIXjXj5R",
    "outputId": "76b6d935-3606-4210-f985-4edad36a8059",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-01-01 00:00:00  to  2022-01-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2086.42it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1941.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-01-31 00:00:00  to  2022-03-02 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1762.25it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2020.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-03-02 00:00:00  to  2022-04-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2093.15it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2099.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2022-04-01 00:00:00  to  2022-05-01 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1467.08it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1862.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 44.4 ms, total: 2.6 s\n",
      "Wall time: 30 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "BANDS = ['VV', 'VH']\n",
    "\n",
    "# Read the points geometries from the CSV table\n",
    "tr_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(tr_afg['Lon'], tr_afg['Lat'])]\n",
    "te_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(te_afg['Lon'], te_afg['Lat'])]\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2022-04-30\", \"%Y-%m-%d\")\n",
    "\n",
    "t = 1\n",
    "while start_date < end_date:\n",
    "    suffix = f'_t{t}'\n",
    "    BANDS_suffixes =[band + suffix for band in BANDS]\n",
    "\n",
    "    tr_afg[BANDS_suffixes] = None\n",
    "    te_afg[BANDS_suffixes] = None\n",
    "\n",
    "    current_date = start_date\n",
    "    next_date = start_date + timedelta(days=30)\n",
    "    print('############', current_date, ' to ', next_date)\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S1_GRD').filterDate(current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "    #Extract train data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(tr_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(tr_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        tr_afg.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "\n",
    "     #Extract test data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(te_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(te_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        te_afg.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "    start_date = next_date\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYLwWCAPXj5S",
    "outputId": "6897a3d9-c29f-4881-f46d-6068e886be33",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-01 00:00:00  to  2019-07-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2133.04it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1758.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-31 00:00:00  to  2019-08-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2068.24it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2073.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-08-30 00:00:00  to  2019-09-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2040.57it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1620.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-09-29 00:00:00  to  2019-10-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1979.49it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2053.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-10-29 00:00:00  to  2019-11-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2048.83it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2056.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-11-28 00:00:00  to  2019-12-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1901.44it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2065.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-12-28 00:00:00  to  2020-01-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2034.87it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1598.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-01-27 00:00:00  to  2020-02-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2051.45it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1945.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-02-26 00:00:00  to  2020-03-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2039.90it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2018.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-03-27 00:00:00  to  2020-04-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1972.82it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1619.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-04-26 00:00:00  to  2020-05-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1979.80it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1184.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-05-26 00:00:00  to  2020-06-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2032.69it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1977.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-06-25 00:00:00  to  2020-07-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1835.58it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1149.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Read the points geometries from the CSV table\n",
    "tr_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(tr_sud['Lon'], tr_sud['Lat'])]\n",
    "te_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(te_sud['Lon'], te_sud['Lat'])]\n",
    "\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(\"2019-07-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2020-06-30\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "t = 1\n",
    "while start_date < end_date:\n",
    "    suffix = f'_t{t}'\n",
    "    BANDS_suffixes =[band + suffix for band in BANDS]\n",
    "\n",
    "    tr_sud[BANDS_suffixes] = None\n",
    "    te_sud[BANDS_suffixes] = None\n",
    "\n",
    "    current_date = start_date\n",
    "    next_date = start_date + timedelta(days=30)\n",
    "    print('############', current_date, ' to ', next_date)\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S1_GRD').filterDate(current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "    #Extract train data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(tr_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(tr_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        tr_sud.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "\n",
    "     #Extract test data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(te_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(te_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        te_sud.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "    start_date = next_date\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrxgIRo0Xj5S",
    "outputId": "ee344c11-937a-47ed-d547-ab6562be0bc1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-01 00:00:00  to  2019-07-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2068.83it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1957.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-07-31 00:00:00  to  2019-08-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2059.96it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2050.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-08-30 00:00:00  to  2019-09-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1634.69it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2042.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-09-29 00:00:00  to  2019-10-29 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2057.27it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2037.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-10-29 00:00:00  to  2019-11-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1834.16it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2072.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-11-28 00:00:00  to  2019-12-28 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1891.27it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2056.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2019-12-28 00:00:00  to  2020-01-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2054.53it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1835.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-01-27 00:00:00  to  2020-02-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1915.71it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2022.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-02-26 00:00:00  to  2020-03-27 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2037.31it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2048.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-03-27 00:00:00  to  2020-04-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1859.89it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1954.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-04-26 00:00:00  to  2020-05-26 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1955.44it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1992.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-05-26 00:00:00  to  2020-06-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1696.66it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 1605.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ 2020-06-25 00:00:00  to  2020-07-25 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2028.91it/s]\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 2021.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.88 s, sys: 134 ms, total: 9.01 s\n",
      "Wall time: 3min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Read the points geometries from the CSV table\n",
    "tr_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(tr_irn['Lon'], tr_irn['Lat'])]\n",
    "te_point_geometries = [ee.Geometry.Point(lon, lat) for lon, lat in zip(te_irn['Lon'], te_irn['Lat'])]\n",
    "\n",
    "\n",
    "start_date = datetime.strptime(\"2019-07-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2020-06-30\", \"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "t = 1\n",
    "while start_date < end_date:\n",
    "    suffix = f'_t{t}'\n",
    "    BANDS_suffixes =[band + suffix for band in BANDS]\n",
    "\n",
    "    tr_irn[BANDS_suffixes] = None\n",
    "    te_irn[BANDS_suffixes] = None\n",
    "\n",
    "    current_date = start_date\n",
    "    next_date = start_date + timedelta(days=30)\n",
    "    print('############', current_date, ' to ', next_date)\n",
    "    collection = (ee.ImageCollection('COPERNICUS/S1_GRD').filterDate(current_date.strftime(\"%Y-%m-%d\"), next_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "\n",
    "    #Extract train data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(tr_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(tr_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        tr_irn.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "\n",
    "     #Extract test data - the mean values for the specified bands\n",
    "    mean_values = (collection.select(BANDS).filterBounds(ee.FeatureCollection(te_point_geometries)).mean().\n",
    "                   reduceRegions(collection=ee.FeatureCollection(te_point_geometries), reducer=ee.Reducer.mean(), scale=10)\n",
    "                  )\n",
    "\n",
    "    i = 0\n",
    "    for feature in tqdm(mean_values.getInfo()['features']):\n",
    "        values = [feature['properties'][band] for band in BANDS]\n",
    "        te_irn.loc[i, BANDS_suffixes] = values\n",
    "        i+=1\n",
    "\n",
    "    start_date = next_date\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "F6_6ddB8Xj5S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_afg.to_csv(f'tr_afg_s1.csv', index=False )\n",
    "te_afg.to_csv(f'te_afg_s1.csv', index=False)\n",
    "\n",
    "tr_sud.to_csv(f'tr_sud_s1.csv', index=False)\n",
    "te_sud.to_csv(f'te_sud_s1.csv', index=False)\n",
    "\n",
    "tr_irn.to_csv(f'tr_irn_s1.csv', index=False)\n",
    "te_irn.to_csv(f'te_irn_s1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf0jiFqJXj5T",
    "tags": []
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLh0nkEUoaET",
    "outputId": "57767a90-70ad-49a6-845e-bd93cb4906d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.3)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.43.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
      "Installing collected packages: catboost\n",
      "Successfully installed catboost-1.2.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyPyXOZxXj5T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Machine Learning\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm.callback import early_stopping, log_evaluation\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "RusOXR1gXj5T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to compute different available vegitation indices from sentinel-2\n",
    "def s2_veg_indices(df, t):\n",
    "    veg_df = pd.DataFrame()\n",
    "    veg_df[f'MNDVI_t{t}'] = (df[f'B8_t{t}'] - df[f'B4_t{t}'] )  / (df[f'B8_t{t}'] +df[f'B4_t{t}'] - 2*df[f'B2_t{t}'])\n",
    "    veg_df[f'NDSI_t{t}'] =  df[f'B3_t{t}']  / (df[f'B11_t{t}'])\n",
    "    veg_df[f'NDVI_t{t}'] =  (df[f'B8_t{t}'] - df[f'B4_t{t}'] )  / (df[f'B8_t{t}'] +df[f'B4_t{t}'] )\n",
    "    veg_df[f'NDWI_t{t}'] = (df[f'B3_t{t}'] -  df[f'B8_t{t}'] )  / (df[f'B3_t{t}'] +df[f'B8_t{t}'])\n",
    "    veg_df[f'NDMI_t{t}'] = (df[f'B8_t{t}'] - df[f'B11_t{t}'] )  / (df[f'B8_t{t}'] +df[f'B11_t{t}' ])\n",
    "    veg_df[f'NDBI_t{t}'] = (df[f'B11_t{t}'] - df[f'B8_t{t}'] )  / (df[f'B11_t{t}'] +df[f'B8_t{t}'])\n",
    "    veg_df[f'NDCI_t{t}'] = (df[f'B5_t{t}'] - df[f'B4_t{t}'] )  / (df[f'B5_t{t}'] +df[f'B4_t{t}'])\n",
    "    veg_df[f'GNDVI_t{t}'] = (df[f'B8_t{t}'] - df[f'B3_t{t}'] )  / (df[f'B8_t{t}'] +df[f'B3_t{t}'] )\n",
    "\n",
    "\n",
    "    veg_df[f'BSI_t{t}'] =  (df[f'B11_t{t}'] - df[f'B4_t{t}'] )  / (df[f'B8_t{t}'] +df[f'B2_t{t}'])\n",
    "    veg_df[f'NDVI_R_t{t}'] =  (df[f'B8_t{t}'] - df[f'B7_t{t}'] )  / (df[f'B8_t{t}'] +df[f'B7_t{t}'])\n",
    "    veg_df[f'CHL_t{t}'] =  (df[f'B7_t{t}'] / (df[f'B5_t{t}']))  - 1\n",
    "    veg_df[f'CVI_t{t}'] =  (df[f'B8_t{t}'] / (df[f'B3_t{t}']))  * (df[f'B4_t{t}'] / (df[f'B3_t{t}']))\n",
    "    veg_df[f'BI_t{t}'] =  (df[f'B4_t{t}'] **2+ df[f'B3_t{t}']**2+ df[f'B2_t{t}']*2) /3\n",
    "    veg_df[f'SI_t{t}'] =  (df[f'B4_t{t}'] - df[f'B2_t{t}'])  / (df[f'B4_t{t}'] + df[f'B2_t{t}'])\n",
    "    veg_df[f'NMDI_t{t}'] =  (df[f'B8_t{t}'] - (df[f'B11_t{t}'] - df[f'B12_t{t}']))  / (df[f'B8_t{t}'] + (df[f'B11_t{t}'] - df[f'B12_t{t}']))\n",
    "    veg_df[f'MSI_t{t}'] =  df[f'B11_t{t}']  / (df[f'B8_t{t}'])\n",
    "    veg_df[f'BSI1_t{t}'] =  ((df[f'B12_t{t}'] + df[f'B4_t{t}']) - (df[f'B8A_t{t}'] + df[f'B2_t{t}']))  / ((df[f'B12_t{t}'] + df[f'B4_t{t}']) + (df[f'B8A_t{t}'] + df[f'B2_t{t}']))\n",
    "    veg_df[f'BSI2_t{t}'] =  ((df[f'B11_t{t}'] + df[f'B4_t{t}']) - (df[f'B8A_t{t}'] + df[f'B2_t{t}']))  / ((df[f'B11_t{t}'] + df[f'B4_t{t}']) + (df[f'B8A_t{t}'] + df[f'B2_t{t}']))\n",
    "    veg_df[f'NDSI1_t{t}'] = (df[f'B11_t{t}'] - df[f'B8A_t{t}'] )  / (df[f'B11_t{t}'] + df[f'B8A_t{t}'] )\n",
    "    veg_df[f'NDSI2_t{t}'] = (df[f'B12_t{t}'] - df[f'B3_t{t}'] )  / (df[f'B12_t{t}'] + df[f'B3_t{t}'] )\n",
    "    veg_df[f'BI2_t{t}'] = df[f'B4_t{t}'] + df[f'B11_t{t}'] - df[f'B8A_t{t}']\n",
    "    veg_df[f'DBSI_t{t}'] = veg_df[f'NDSI2_t{t}'] - ((df[f'B8A_t{t}'] - df[f'B4_t{t}']) / (df[f'B8A_t{t}'] + df[f'B4_t{t}']))\n",
    "    veg_df[f'MBI_t{t}'] = ((df[f'B11_t{t}'] + df[f'B12_t{t}'] + df[f'B8A_t{t}']) / (df[f'B11_t{t}'] + df[f'B12_t{t}'] + df[f'B8A_t{t}'])) + 0.5\n",
    "    veg_df[f'R03_t{t}'] =  df[f'B11_t{t}']  / (df[f'B12_t{t}'])\n",
    "    veg_df[f'R04_t{t}'] =  df[f'B5_t{t}']  / (df[f'B4_t{t}'])\n",
    "    veg_df[f'MI_t{t}'] = (df[f'B8A_t{t}'] - df[f'B11_t{t}'] )  / (df[f'B8A_t{t}'] + df[f'B11_t{t}'] )\n",
    "    veg_df[f'PSRI_t{t}'] = (df[f'B4_t{t}'] - df[f'B2_t{t}'] )  / (df[f'B6_t{t}'] )\n",
    "    veg_df[f'TVI_t{t}'] = (120*(df[f'B6_t{t}'] - df[f'B3_t{t}'] ) - 200 * (df[f'B4_t{t}'] - df[f'B3_t{t}'])) / 2\n",
    "    veg_df[f'ARVI_t{t}'] = (df[f'B8_t{t}'] - 2*df[f'B4_t{t}'] + df[f'B2_t{t}'])  / (df[f'B8_t{t}'] + 2*df[f'B4_t{t}'] + df[f'B2_t{t}'])\n",
    "    veg_df[f'SIPI_t{t}'] =  (df[f'B8_t{t}'] - df[f'B2_t{t}'] )  / (df[f'B8_t{t}'] +df[f'B4_t{t}'] )\n",
    "    veg_df[f'EXG_t{t}'] = (2 * df[f'B3_t{t}'] -  df[f'B4_t{t}'] -  df[f'B2_t{t}'] )\n",
    "    veg_df[f'ACI_t{t}'] = (df[f'B8_t{t}']  )  * (df[f'B4_t{t}'] +df[f'B3_t{t}'] )\n",
    "\n",
    "    # REDEDGE indices\n",
    "    veg_df[f'NDVIre1_t{t}'] =  (df[f'B8_t{t}'] - df[f'B5_t{t}'])  / (df[f'B8_t{t}'] + df[f'B5_t{t}'])\n",
    "    veg_df[f'NDVIre2_t{t}'] =  (df[f'B8_t{t}'] - df[f'B6_t{t}'])  / (df[f'B8_t{t}'] + df[f'B6_t{t}'])\n",
    "    veg_df[f'NDVIre3_t{t}'] =  (df[f'B8_t{t}'] - df[f'B7_t{t}'])  / (df[f'B8_t{t}'] + df[f'B7_t{t}'])\n",
    "\n",
    "    veg_df[f'NDRE1_t{t}'] =  (df[f'B6_t{t}'] - df[f'B5_t{t}'])  / (df[f'B6_t{t}'] + df[f'B5_t{t}'])\n",
    "    veg_df[f'NDRE2_t{t}'] =  (df[f'B7_t{t}'] - df[f'B5_t{t}'])  / (df[f'B7_t{t}'] + df[f'B5_t{t}'])\n",
    "    veg_df[f'NDRE3_t{t}'] =  (df[f'B7_t{t}'] - df[f'B6_t{t}'])  / (df[f'B7_t{t}'] + df[f'B6_t{t}'])\n",
    "\n",
    "    veg_df[f'CIre1_t{t}'] =  (df[f'B8_t{t}'] /(df[f'B5_t{t}']))  - 1\n",
    "    veg_df[f'CIre2_t{t}'] =  (df[f'B8_t{t}'] /(df[f'B6_t{t}']))  - 1\n",
    "    veg_df[f'CIre3_t{t}'] =  (df[f'B8_t{t}'] /(df[f'B7_t{t}']))  - 1\n",
    "\n",
    "    veg_df[f'MCARI1_t{t}'] =  ((df[f'B5_t{t}'] - df[f'B4_t{t}']) - 0.2*(df[f'B5_t{t}'] - df[f'B3_t{t}'])) * (df[f'B5_t{t}'] / (df[f'B4_t{t}']))\n",
    "    veg_df[f'MCARI2_t{t}'] =  ((df[f'B6_t{t}'] - df[f'B4_t{t}']) - 0.2*(df[f'B6_t{t}'] - df[f'B3_t{t}'])) * (df[f'B6_t{t}'] / (df[f'B4_t{t}']))\n",
    "    veg_df[f'MCARI3_t{t}'] =  ((df[f'B7_t{t}'] - df[f'B4_t{t}']) - 0.2*(df[f'B7_t{t}'] - df[f'B3_t{t}'])) * (df[f'B7_t{t}'] / (df[f'B4_t{t}']))\n",
    "\n",
    "\n",
    "    veg_df[f'TCARI1_t{t}'] =  3*((df[f'B5_t{t}'] - df[f'B4_t{t}']) - 0.2*(df[f'B5_t{t}'] - df[f'B3_t{t}'])) * (df[f'B5_t{t}'] / (df[f'B4_t{t}']))\n",
    "    veg_df[f'TCARI2_t{t}'] =  3*((df[f'B6_t{t}'] - df[f'B4_t{t}']) - 0.2*(df[f'B6_t{t}'] - df[f'B3_t{t}'])) * (df[f'B6_t{t}'] / (df[f'B4_t{t}']))\n",
    "    veg_df[f'TCARI3_t{t}'] =  3*((df[f'B7_t{t}'] - df[f'B4_t{t}']) - 0.2*(df[f'B7_t{t}'] - df[f'B3_t{t}'])) * (df[f'B7_t{t}'] / (df[f'B4_t{t}']))\n",
    "\n",
    "    veg_df[f'MTCI1_t{t}'] =  (df[f'B6_t{t}'] - df[f'B5_t{t}'])  / (df[f'B5_t{t}'] - df[f'B4_t{t}'] + 1e-6)\n",
    "    veg_df[f'MTCI2_t{t}'] =  (df[f'B7_t{t}'] - df[f'B5_t{t}'])  / (df[f'B5_t{t}'] - df[f'B4_t{t}']+ 1e-6)\n",
    "    veg_df[f'MTCI3_t{t}'] =  (df[f'B7_t{t}'] - df[f'B6_t{t}'])  / (df[f'B6_t{t}'] - df[f'B4_t{t}']+ 1e-6)\n",
    "\n",
    "    # Blooming Indices (to detect flowers colors (purple, yellow) of different crops)\n",
    "    veg_df[f'NDGI_t{t}'] =  (df[f'B4_t{t}'] - df[f'B3_t{t}'] )  / (df[f'B4_t{t}'] +df[f'B3_t{t}'] )\n",
    "    veg_df[f'DYI_t{t}'] =  df[f'B4_t{t}']  / df[f'B3_t{t}']\n",
    "    veg_df[f'NDPI_t{t}'] =  (0.5*(df[f'B4_t{t}'] + df[f'B2_t{t}']) - df[f'B3_t{t}'])  / (0.5*(df[f'B4_t{t}'] + df[f'B2_t{t}']) + df[f'B3_t{t}'])\n",
    "    veg_df[f'PEBI_t{t}'] =  veg_df[f'NDPI_t{t}'] / ((veg_df[f'NDGI_t{t}'] +1) * df[f'B8_t{t}'])\n",
    "    veg_df[f'NDYI_t{t}'] =  (0.5*(df[f'B4_t{t}'] + df[f'B3_t{t}']) - df[f'B2_t{t}'])  / (0.5*(df[f'B4_t{t}'] + df[f'B3_t{t}']) + df[f'B2_t{t}'])\n",
    "    veg_df[f'YEBI_t{t}'] =  veg_df[f'NDYI_t{t}'] / ((veg_df[f'NDGI_t{t}'] +1) * df[f'B8_t{t}'])\n",
    "\n",
    "\n",
    "    veg_df['ID'] = list(df['ID'])\n",
    "\n",
    "    return veg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AmbF1O0oXj5T",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Function to compute different available vegitation indices from sentinel-1\n",
    "def s1_veg_indices(df, t):\n",
    "    veg_df = pd.DataFrame()\n",
    "\n",
    "    veg_df[f'MNDVI_t{t}'] = df[f'VV_t{t}'] /df[f'VH_t{t}']\n",
    "    RVI = (4*df[f'VH_t{t}']) / (df[f'VV_t{t}'] + df[f'VH_t{t}'])\n",
    "    veg_df[f'RVI_t{t}'] =  list(RVI)\n",
    "\n",
    "    veg_df[f'VV*VH_t{t}'] = df[f'VV_t{t}'] * df[f'VH_t{t}']\n",
    "\n",
    "    DOP = df[f'VV_t{t}'] / (df[f'VV_t{t}'] + df[f'VH_t{t}'])\n",
    "    RVI4 = list(np.sqrt(DOP) * RVI)\n",
    "    veg_df[f'RVI4_t{t}'] = list(RVI4)\n",
    "\n",
    "\n",
    "    veg_df['ID'] = list(df['ID'])\n",
    "    return veg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_qRUF6LXj5T",
    "tags": []
   },
   "source": [
    "### Afghanistan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VL5ABcRwXj5U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read teh extractd data from the 3 different sources\n",
    "\n",
    "## Sentinel-2\n",
    "tr_afg_s2 = pd.read_csv(f'tr_afg_s2.csv')\n",
    "te_afg_s2 = pd.read_csv(f'te_afg_s2.csv')\n",
    "\n",
    "## Sentinel-1\n",
    "tr_afg_s1 = pd.read_csv(f'tr_afg_s1.csv')\n",
    "te_afg_s1 = pd.read_csv(f'te_afg_s1.csv')\n",
    "\n",
    "## SMAP\n",
    "tr_afg_smap = pd.read_csv(f'tr_afg_smap.csv')\n",
    "te_afg_smap = pd.read_csv(f'te_afg_smap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "teXRKGjAXj5U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the sentinel-2 vegitation indecies for the 4 months\n",
    "for time in range(4):\n",
    "    tr_veg_indices = s2_veg_indices(tr_afg_s2, t= time+1)\n",
    "    tr_afg_s2 = pd.merge(tr_afg_s2, tr_veg_indices, on=['ID'], how='inner')\n",
    "\n",
    "    te_veg_indices = s2_veg_indices(te_afg_s2, t= time+1)\n",
    "    te_afg_s2 = pd.merge(te_afg_s2, te_veg_indices, on=['ID'], how='inner')\n",
    "\n",
    "\n",
    "# Compute the sentinel-1 vegitation indecies for the 4 months\n",
    "for time in range(4):\n",
    "    tr_veg_indices = s1_veg_indices(tr_afg_s1, t= time+1)\n",
    "    tr_afg_s1 = pd.merge(tr_afg_s1, tr_veg_indices, on=['ID'], how='inner')\n",
    "\n",
    "    te_veg_indices = s1_veg_indices(te_afg_s1, t= time+1)\n",
    "    te_afg_s1 = pd.merge(te_afg_s1, te_veg_indices, on=['ID'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sLkVKDv_Xj5U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the computed vegetation indecies and the other extracted data into 1 dataset\n",
    "tr_afg = pd.merge(tr_afg_s2, tr_afg_s1, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster','Target'], how='inner')\n",
    "tr_afg = pd.merge(tr_afg, tr_afg_smap, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster','Target'], how='inner')\n",
    "\n",
    "\n",
    "te_afg = pd.merge(te_afg_s2, te_afg_s1, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster'], how='inner')\n",
    "te_afg = pd.merge(te_afg, te_afg_smap, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvmwoV63Xj5U",
    "outputId": "2e4f22fd-a04e-442d-f363-326d17907646",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 320)\n",
      "(500, 320)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "X = tr_afg.drop(['ID', 'Target', 'geometry', 'Cluster', 'Lat', 'Lon'], axis = 1).fillna(-99999)\n",
    "y = tr_afg.Target\n",
    "test_df = te_afg.drop(['ID', 'geometry', 'Cluster' , 'Lat', 'Lon'], axis = 1).fillna(-99999)\n",
    "\n",
    "print(X.shape)\n",
    "print(test_df.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C32ya7CiXj5U",
    "outputId": "e0780f98-2991-4863-a553-f1a1f8318b46",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Fold number 1 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.443621\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid_0's binary_logloss: 0.337541\n",
      "Accuracy Score: 0.87\n",
      "########### Fold number 2 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.628191\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's binary_logloss: 0.364412\n",
      "Accuracy Score: 0.86\n",
      "########### Fold number 3 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.494832\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid_0's binary_logloss: 0.370716\n",
      "Accuracy Score: 0.84\n",
      "########### Fold number 4 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.606033\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's binary_logloss: 0.381801\n",
      "Accuracy Score: 0.84\n",
      "########### Fold number 5 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.507346\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's binary_logloss: 0.313256\n",
      "Accuracy Score: 0.9\n",
      "Mean accuracy: 0.8619999999999999\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=SEED) # for cross validation\n",
    "afg_lgbmscores = []\n",
    "afg_lgbmpreds= []\n",
    "\n",
    "\n",
    "# Creating loop for the stratified k fold\n",
    "i = 0\n",
    "for train, val in skf.split(X, y):\n",
    "    print(f'########### Fold number {i+1} ')\n",
    "\n",
    "    # spliting the data\n",
    "    x_train, x_val = X.iloc[train], X.iloc[val]\n",
    "    y_train, y_val = y.iloc[train], y.iloc[val]\n",
    "\n",
    "    clf = LGBMClassifier(boosting_type='gbdt',learning_rate=0.08,\n",
    "                           n_estimators=2000,deterministic=True, objective='binary',\n",
    "                           subsample=0.90, subsample_freq=5,\n",
    "                           random_state=SEED,n_jobs=- 1)\n",
    "\n",
    "    # fitting on train data\n",
    "    clf.fit( x_train, y_train, eval_set = (x_val,y_val), callbacks=[log_evaluation(period=150), early_stopping(200)])\n",
    "\n",
    "    # Making predictions\n",
    "    y_pred = clf.predict(x_val)\n",
    "\n",
    "    # Measuring the accuracy of the model\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy Score: {score}')\n",
    "    afg_lgbmscores.append(score)\n",
    "\n",
    "    preds = clf.predict_proba(test_df)\n",
    "    afg_lgbmpreds.append(preds)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print(f'Mean accuracy: {np.mean(afg_lgbmscores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "u1ohgBYjXj5V",
    "tags": []
   },
   "outputs": [],
   "source": [
    "afg_lgbmpreds_mean = np.mean(afg_lgbmpreds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wnrox0bTXj5Z",
    "outputId": "fee03f59-df19-4664-bab9-05f6c79a848b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Fold number 1 \n",
      "0:\tlearn: 0.6395486\ttest: 0.6563229\tbest: 0.6563229 (0)\ttotal: 55.7ms\tremaining: 27m 52s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.3469863027\n",
      "bestIteration = 99\n",
      "\n",
      "Shrink model to first 100 iterations.\n",
      "Accuracy Score: 0.85\n",
      "########### Fold number 2 \n",
      "0:\tlearn: 0.6351133\ttest: 0.6426304\tbest: 0.6426304 (0)\ttotal: 13.1ms\tremaining: 6m 34s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.3712185627\n",
      "bestIteration = 27\n",
      "\n",
      "Shrink model to first 28 iterations.\n",
      "Accuracy Score: 0.86\n",
      "########### Fold number 3 \n",
      "0:\tlearn: 0.6411887\ttest: 0.6552670\tbest: 0.6552670 (0)\ttotal: 6.73ms\tremaining: 3m 21s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.3303030595\n",
      "bestIteration = 80\n",
      "\n",
      "Shrink model to first 81 iterations.\n",
      "Accuracy Score: 0.86\n",
      "########### Fold number 4 \n",
      "0:\tlearn: 0.6385482\ttest: 0.6623217\tbest: 0.6623217 (0)\ttotal: 7.18ms\tremaining: 3m 35s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.414928206\n",
      "bestIteration = 47\n",
      "\n",
      "Shrink model to first 48 iterations.\n",
      "Accuracy Score: 0.8\n",
      "########### Fold number 5 \n",
      "0:\tlearn: 0.6371865\ttest: 0.6486801\tbest: 0.6486801 (0)\ttotal: 12.4ms\tremaining: 6m 11s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.3112681825\n",
      "bestIteration = 42\n",
      "\n",
      "Shrink model to first 43 iterations.\n",
      "Accuracy Score: 0.89\n",
      "Mean accuracy: 0.852\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=SEED) # for cross validation\n",
    "afg_catscores = []\n",
    "afg_catpreds= []\n",
    "\n",
    "\n",
    "# Creating loop for the stratified k fold\n",
    "i = 0\n",
    "for train, val in skf.split(X, y):\n",
    "    print(f'########### Fold number {i+1} ')\n",
    "\n",
    "    # spliting the data\n",
    "    x_train, x_val = X.iloc[train], X.iloc[val]\n",
    "    y_train, y_val = y.iloc[train], y.iloc[val]\n",
    "\n",
    "    clf = CatBoostClassifier(iterations=30000,  has_time=True ,bootstrap_type='No',random_strength=0,\n",
    "                                   learning_rate=0.05,use_best_model=True,\n",
    "                                   random_seed=SEED)\n",
    "    # fitting on train data\n",
    "    clf.fit( x_train, y_train, eval_set = (x_val,y_val),verbose=500 ,early_stopping_rounds=300)\n",
    "\n",
    "    # Making predictions\n",
    "    y_pred = clf.predict(x_val)\n",
    "\n",
    "    # Measuring the accuracy of the model\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy Score: {score}')\n",
    "    afg_catscores.append(score)\n",
    "\n",
    "    preds = clf.predict_proba(test_df)\n",
    "    afg_catpreds.append(preds)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print(f'Mean accuracy: {np.mean(afg_catscores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "j76L-uLwXj5a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "afg_catpreds_mean = np.mean(afg_catpreds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "i_dMZ9FXXj5a",
    "outputId": "d4c16091-1d82-431a-fb26-8aebed918b70",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_9ZLHTVF6NSU7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_B2WO8GOJOMY1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_K82JJ5PQCMXM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_5LACW9CE2OIB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_O6RIQWT1103E</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  Target\n",
       "0  ID_9ZLHTVF6NSU7       1\n",
       "1  ID_B2WO8GOJOMY1       0\n",
       "2  ID_K82JJ5PQCMXM       1\n",
       "3  ID_5LACW9CE2OIB       1\n",
       "4  ID_O6RIQWT1103E       0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_afg = 0.40*afg_catpreds_mean + 0.60*afg_lgbmpreds_mean\n",
    "\n",
    "blend_afg = np.argmax(blend_afg, axis=1)\n",
    "blend_afg = pd.DataFrame({'ID': te_afg.ID, 'Target': blend_afg})\n",
    "blend_afg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITb0EvmnXj5a"
   },
   "source": [
    "### Sudan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "MycThHk3Xj5a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read teh extractd data from the 3 different sources\n",
    "\n",
    "## Sentinel-2\n",
    "tr_sud_s2 = pd.read_csv(f'tr_sud_s2.csv')\n",
    "te_sud_s2 = pd.read_csv(f'te_sud_s2.csv')\n",
    "\n",
    "## Sentinel-1\n",
    "tr_sud_s1 = pd.read_csv(f'tr_sud_s1.csv')\n",
    "te_sud_s1 = pd.read_csv(f'te_sud_s1.csv')\n",
    "\n",
    "## SMAP\n",
    "tr_sud_smap = pd.read_csv(f'tr_sud_smap.csv')\n",
    "te_sud_smap = pd.read_csv(f'te_sud_smap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "161lEj9uXj5a",
    "outputId": "7627a846-14ed-43d1-c73f-bd8e1f28543c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Compute the sentinel-2 vegitation indecies for the entire timeseries\n",
    "for time in range(13):\n",
    "    tr_veg_indices = s2_veg_indices(tr_sud_s2, t= time+1)\n",
    "    tr_sud_s2 = pd.merge(tr_sud_s2, tr_veg_indices, on=['ID'], how='inner')\n",
    "\n",
    "    te_veg_indices = s2_veg_indices(te_sud_s2, t= time+1)\n",
    "    te_sud_s2 = pd.merge(te_sud_s2, te_veg_indices, on=['ID'], how='inner')\n",
    "\n",
    "\n",
    "# Compute the sentinel-1 vegitation indecies for the entire timeseries\n",
    "for time in range(13):\n",
    "    tr_veg_indices = s1_veg_indices(tr_sud_s1, t= time+1)\n",
    "    tr_sud_s1 = pd.merge(tr_sud_s1, tr_veg_indices, on=['ID'], how='inner')\n",
    "\n",
    "    te_veg_indices = s1_veg_indices(te_sud_s1, t= time+1)\n",
    "    te_sud_s1 = pd.merge(te_sud_s1, te_veg_indices, on=['ID'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jfQYWMl0Xj5a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the computed vegetation indecies and the other extracted data into 1 dataset\n",
    "tr_sud = pd.merge(tr_sud_s2, tr_sud_s1, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster','Target'], how='inner')\n",
    "tr_sud = pd.merge(tr_sud, tr_sud_smap, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster','Target'], how='inner')\n",
    "\n",
    "te_sud = pd.merge(te_sud_s2, te_sud_s1, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster'], how='inner')\n",
    "te_sud = pd.merge(te_sud, te_sud_smap, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XH-xvIxtXj5b",
    "outputId": "a6384281-83b9-48e4-deed-ec4dce996e94",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1040)\n",
      "(500, 1040)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "X = tr_sud.drop(['ID', 'Target', 'geometry', 'Cluster', 'Lat', 'Lon'], axis = 1).fillna(-99999)\n",
    "y = tr_sud.Target\n",
    "test_df = te_sud.drop(['ID', 'geometry', 'Cluster', 'Lat', 'Lon' ], axis = 1).fillna(-99999)\n",
    "\n",
    "print(X.shape)\n",
    "print(test_df.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhbvNuKgXj5b",
    "outputId": "a7829e25-33e1-4459-faec-daffc9c5a24c",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Fold number 1 \n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.131538\n",
      "[300]\tvalid_0's binary_logloss: 0.122632\n",
      "[450]\tvalid_0's binary_logloss: 0.124889\n",
      "Early stopping, best iteration is:\n",
      "[264]\tvalid_0's binary_logloss: 0.110848\n",
      "Accuracy Score: 0.96\n",
      "########### Fold number 2 \n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.100499\n",
      "[300]\tvalid_0's binary_logloss: 0.0742387\n",
      "[450]\tvalid_0's binary_logloss: 0.0781716\n",
      "Early stopping, best iteration is:\n",
      "[274]\tvalid_0's binary_logloss: 0.0640944\n",
      "Accuracy Score: 0.97\n",
      "########### Fold number 3 \n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.0958398\n",
      "[300]\tvalid_0's binary_logloss: 0.106639\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's binary_logloss: 0.0884784\n",
      "Accuracy Score: 0.96\n",
      "########### Fold number 4 \n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.00763529\n",
      "[300]\tvalid_0's binary_logloss: 0.000899186\n",
      "[450]\tvalid_0's binary_logloss: 0.000632989\n",
      "[600]\tvalid_0's binary_logloss: 0.000651632\n",
      "[750]\tvalid_0's binary_logloss: 0.000651632\n",
      "Early stopping, best iteration is:\n",
      "[460]\tvalid_0's binary_logloss: 0.000601371\n",
      "Accuracy Score: 1.0\n",
      "########### Fold number 5 \n",
      "Training until validation scores don't improve for 300 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.0340083\n",
      "[300]\tvalid_0's binary_logloss: 0.0290671\n",
      "[450]\tvalid_0's binary_logloss: 0.0360372\n",
      "Early stopping, best iteration is:\n",
      "[233]\tvalid_0's binary_logloss: 0.0195026\n",
      "Accuracy Score: 0.99\n",
      "Mean accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=SEED) # for cross validation\n",
    "sud_lgbmscores = []\n",
    "sud_lgbmpreds= []\n",
    "\n",
    "\n",
    "# Creating loop for the stratified k fold\n",
    "i = 0\n",
    "for train, val in skf.split(X, y):\n",
    "    print(f'########### Fold number {i+1} ')\n",
    "\n",
    "    # spliting the data\n",
    "    x_train, x_val = X.iloc[train], X.iloc[val]\n",
    "    y_train, y_val = y.iloc[train], y.iloc[val]\n",
    "\n",
    "    clf = LGBMClassifier(boosting_type='gbdt',learning_rate=0.05, #0.05\n",
    "                         n_estimators=2000,deterministic=True, objective='binary',\n",
    "                         subsample=0.90, subsample_freq=5,\n",
    "                         random_state=SEED,n_jobs=- 1)\n",
    "\n",
    "    # fitting on train data\n",
    "    clf.fit( x_train, y_train, eval_set = (x_val,y_val),\n",
    "            callbacks=[log_evaluation(period=150), early_stopping(300)])\n",
    "\n",
    "    # Making predictions\n",
    "    y_pred = clf.predict(x_val)\n",
    "\n",
    "    # Measuring the accuracy of the model\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy Score: {score}')\n",
    "    sud_lgbmscores.append(score)\n",
    "\n",
    "    preds = clf.predict_proba(test_df)\n",
    "    sud_lgbmpreds.append(preds)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print(f'Mean accuracy: {np.mean(sud_lgbmscores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "CgF-ryEJXj5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sud_lgbmpreds_mean = np.mean(sud_lgbmpreds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPh2jlD0Xj5c",
    "outputId": "7db2e1d9-6d51-48f6-f70b-b5fdeb150df9",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Fold number 1 \n",
      "0:\tlearn: 0.6742880\ttest: 0.6785644\tbest: 0.6785644 (0)\ttotal: 22.1ms\tremaining: 11m 1s\n",
      "500:\tlearn: 0.0059129\ttest: 0.1276669\tbest: 0.1243471 (375)\ttotal: 11.1s\tremaining: 10m 53s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1243470954\n",
      "bestIteration = 375\n",
      "\n",
      "Shrink model to first 376 iterations.\n",
      "Accuracy Score: 0.95\n",
      "########### Fold number 2 \n",
      "0:\tlearn: 0.6763387\ttest: 0.6782827\tbest: 0.6782827 (0)\ttotal: 20.1ms\tremaining: 10m 1s\n",
      "500:\tlearn: 0.0051574\ttest: 0.0954140\tbest: 0.0954140 (500)\ttotal: 11s\tremaining: 10m 50s\n",
      "1000:\tlearn: 0.0023337\ttest: 0.0884422\tbest: 0.0884422 (1000)\ttotal: 21.6s\tremaining: 10m 25s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.08750726767\n",
      "bestIteration = 1085\n",
      "\n",
      "Shrink model to first 1086 iterations.\n",
      "Accuracy Score: 0.95\n",
      "########### Fold number 3 \n",
      "0:\tlearn: 0.6756061\ttest: 0.6774059\tbest: 0.6774059 (0)\ttotal: 32.1ms\tremaining: 16m 4s\n",
      "500:\tlearn: 0.0060991\ttest: 0.1161138\tbest: 0.1068479 (257)\ttotal: 10.9s\tremaining: 10m 40s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.10684795\n",
      "bestIteration = 257\n",
      "\n",
      "Shrink model to first 258 iterations.\n",
      "Accuracy Score: 0.96\n",
      "########### Fold number 4 \n",
      "0:\tlearn: 0.6751041\ttest: 0.6755380\tbest: 0.6755380 (0)\ttotal: 22.4ms\tremaining: 11m 12s\n",
      "500:\tlearn: 0.0061129\ttest: 0.0165758\tbest: 0.0165758 (500)\ttotal: 11.6s\tremaining: 11m 23s\n",
      "1000:\tlearn: 0.0022491\ttest: 0.0113740\tbest: 0.0113740 (1000)\ttotal: 22.8s\tremaining: 10m 59s\n",
      "1500:\tlearn: 0.0017024\ttest: 0.0105107\tbest: 0.0105107 (1348)\ttotal: 33.7s\tremaining: 10m 39s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.01051068215\n",
      "bestIteration = 1348\n",
      "\n",
      "Shrink model to first 1349 iterations.\n",
      "Accuracy Score: 1.0\n",
      "########### Fold number 5 \n",
      "0:\tlearn: 0.6736710\ttest: 0.6754654\tbest: 0.6754654 (0)\ttotal: 23.8ms\tremaining: 11m 53s\n",
      "500:\tlearn: 0.0059681\ttest: 0.0644134\tbest: 0.0644134 (500)\ttotal: 11.1s\tremaining: 10m 53s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.06336914819\n",
      "bestIteration = 530\n",
      "\n",
      "Shrink model to first 531 iterations.\n",
      "Accuracy Score: 0.97\n",
      "Mean accuracy: 0.966\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=SEED) # for cross validation\n",
    "sud_catscores = []\n",
    "sud_catpreds= []\n",
    "\n",
    "\n",
    "# Creating loop for the stratified k fold\n",
    "i = 0\n",
    "for train, val in skf.split(X, y):\n",
    "    print(f'########### Fold number {i+1} ')\n",
    "\n",
    "    # spliting the data\n",
    "    x_train, x_val = X.iloc[train], X.iloc[val]\n",
    "    y_train, y_val = y.iloc[train], y.iloc[val]\n",
    "\n",
    "    clf = CatBoostClassifier(iterations=30000,  has_time=True ,bootstrap_type='No',\n",
    "                             random_strength=0,\n",
    "                             learning_rate=0.01,use_best_model=True,#0.08\n",
    "                             random_seed=SEED)\n",
    "\n",
    "    # fitting on train data\n",
    "    clf.fit( x_train, y_train, eval_set = (x_val,y_val),verbose=500 ,early_stopping_rounds=300)\n",
    "\n",
    "    # Making predictions\n",
    "    y_pred = clf.predict(x_val)\n",
    "\n",
    "    # Measuring the accuracy of the model\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy Score: {score}')\n",
    "    sud_catscores.append(score)\n",
    "\n",
    "    preds = clf.predict_proba(test_df)\n",
    "    sud_catpreds.append(preds)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print(f'Mean accuracy: {np.mean(sud_catscores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "HP26ugBMXj5c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sud_catpreds_mean = np.mean(sud_catpreds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6h4lc3HrXj5c",
    "outputId": "0c39daee-0eaa-446f-b2e7-af1cc520407f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_SOYSG7W04UH3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_EAP7EXXV8ZDE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_QPRX1TUQVGHU</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_C78YQ32G1KO9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_M5X39UIEM64N</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  Target\n",
       "0  ID_SOYSG7W04UH3       1\n",
       "1  ID_EAP7EXXV8ZDE       1\n",
       "2  ID_QPRX1TUQVGHU       0\n",
       "3  ID_C78YQ32G1KO9       0\n",
       "4  ID_M5X39UIEM64N       1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_sud = 0.55*sud_lgbmpreds_mean + 0.45*sud_catpreds_mean\n",
    "\n",
    "blend_sud = np.argmax(blend_sud, axis=1)\n",
    "blend_sud = pd.DataFrame({'ID': te_sud.ID, 'Target': blend_sud})\n",
    "blend_sud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLh8nxkFXj5c"
   },
   "source": [
    "### Iran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "dJ8MKvqPXj5c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read teh extractd data from the 3 different sources\n",
    "\n",
    "## Sentinel-2\n",
    "tr_irn_s2 = pd.read_csv(f'tr_irn_s2.csv')\n",
    "te_irn_s2 = pd.read_csv(f'te_irn_s2.csv')\n",
    "\n",
    "## Sentinel-1\n",
    "tr_irn_s1 = pd.read_csv(f'tr_irn_s1.csv')\n",
    "te_irn_s1 = pd.read_csv(f'te_irn_s1.csv')\n",
    "\n",
    "## SMAP\n",
    "tr_irn_smap = pd.read_csv(f'tr_irn_smap.csv')\n",
    "te_irn_smap = pd.read_csv(f'te_irn_smap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXchlQUQXj5d",
    "outputId": "cb42f5ef-086b-44df-bd6c-14fd98c45cfe",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/conda/envs/eurac_env/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Compute the sentinel-2 vegitation indecies for the entire timeseries\n",
    "for time in range(13):\n",
    "    tr_veg_indices = s2_veg_indices(tr_irn_s2, t= time+1)\n",
    "    tr_irn_s2 = pd.merge(tr_irn_s2, tr_veg_indices, on=['ID'], how='inner')\n",
    "\n",
    "    te_veg_indices = s2_veg_indices(te_irn_s2, t= time+1)\n",
    "    te_irn_s2 = pd.merge(te_irn_s2, te_veg_indices, on=['ID'], how='inner')\n",
    "\n",
    "\n",
    "# Compute the sentinel-1 vegitation indecies for the entire timeseries\n",
    "for time in range(13):\n",
    "    tr_veg_indices = s1_veg_indices(tr_irn_s1, t= time+1)\n",
    "    tr_irn_s1 = pd.merge(tr_irn_s1, tr_veg_indices, on=['ID'], how='inner')\n",
    "\n",
    "    te_veg_indices = s1_veg_indices(te_irn_s1, t= time+1)\n",
    "    te_irn_s1 = pd.merge(te_irn_s1, te_veg_indices, on=['ID'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "K-fJUkgWXj5d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the computed vegetation indecies and the other extracted data into 1 dataset\n",
    "tr_irn = pd.merge(tr_irn_s2, tr_irn_s1, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster','Target'], how='inner')\n",
    "tr_irn = pd.merge(tr_irn, tr_irn_smap, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster','Target'], how='inner')\n",
    "\n",
    "te_irn = pd.merge(te_irn_s2, te_irn_s1, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster'], how='inner')\n",
    "te_irn = pd.merge(te_irn, te_irn_smap, on=['ID', 'Lat', 'Lon', 'geometry', 'Cluster'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9M395gfLXj5d",
    "outputId": "18203f47-2238-488f-8489-e14a94162fe4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1040)\n",
      "(500, 1040)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "X = tr_irn.drop(['ID', 'Target', 'geometry', 'Cluster', 'Lat', 'Lon'], axis = 1).fillna(-99999)\n",
    "y = tr_irn.Target\n",
    "test_df = te_irn.drop(['ID', 'geometry', 'Cluster', 'Lat', 'Lon' ], axis = 1).fillna(-99999)\n",
    "\n",
    "print(X.shape)\n",
    "print(test_df.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jwv4GQ5sXj5d",
    "outputId": "f329cec4-59f9-4045-a2b0-ce920510d310",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Fold number 1 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.0807749\n",
      "[300]\tvalid_0's binary_logloss: 0.11461\n",
      "Early stopping, best iteration is:\n",
      "[194]\tvalid_0's binary_logloss: 0.0732114\n",
      "Accuracy Score: 0.97\n",
      "########### Fold number 2 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.10424\n",
      "[300]\tvalid_0's binary_logloss: 0.140228\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's binary_logloss: 0.0984734\n",
      "Accuracy Score: 0.99\n",
      "########### Fold number 3 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.220543\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's binary_logloss: 0.174141\n",
      "Accuracy Score: 0.96\n",
      "########### Fold number 4 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.103944\n",
      "[300]\tvalid_0's binary_logloss: 0.118805\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid_0's binary_logloss: 0.0963447\n",
      "Accuracy Score: 0.97\n",
      "########### Fold number 5 \n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\tvalid_0's binary_logloss: 0.203577\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's binary_logloss: 0.16463\n",
      "Accuracy Score: 0.96\n",
      "Mean accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=SEED) # for cross validation\n",
    "irn_lgbmscores = []\n",
    "irn_lgbmpreds= []\n",
    "\n",
    "\n",
    "# Creating loop for the stratified k fold\n",
    "i = 0\n",
    "for train, val in skf.split(X, y):\n",
    "    print(f'########### Fold number {i+1} ')\n",
    "\n",
    "    # spliting the data\n",
    "    x_train, x_val = X.iloc[train], X.iloc[val]\n",
    "    y_train, y_val = y.iloc[train], y.iloc[val]\n",
    "\n",
    "    clf = LGBMClassifier(boosting_type='gbdt',learning_rate=0.05,#0.08\n",
    "                         n_estimators=2000,deterministic=True, objective='binary',\n",
    "                         subsample=0.90, subsample_freq=10,\n",
    "                         random_state=SEED,n_jobs=- 1)\n",
    "\n",
    "    # fitting on train data\n",
    "    clf.fit( x_train, y_train, eval_set = (x_val,y_val), callbacks=[log_evaluation(period=150), early_stopping(200)])\n",
    "\n",
    "    # Making predictions\n",
    "    y_pred = clf.predict(x_val)\n",
    "\n",
    "    # Measuring the accuracy of the model\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy Score: {score}')\n",
    "    irn_lgbmscores.append(score)\n",
    "\n",
    "    preds = clf.predict_proba(test_df)\n",
    "    irn_lgbmpreds.append(preds)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print(f'Mean accuracy: {np.mean(irn_lgbmscores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "LlJixIxwXj5e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "irn_lgbmpreds_mean = np.mean(irn_lgbmpreds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oS0gLZEKXj5e",
    "outputId": "43acdd15-4ba9-4669-dea6-3d794f4c19b5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Fold number 1 \n",
      "0:\tlearn: 0.5517591\ttest: 0.5633655\tbest: 0.5633655 (0)\ttotal: 21.1ms\tremaining: 10m 32s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.0905574612\n",
      "bestIteration = 90\n",
      "\n",
      "Shrink model to first 91 iterations.\n",
      "Accuracy Score: 0.97\n",
      "########### Fold number 2 \n",
      "0:\tlearn: 0.5449474\ttest: 0.5635683\tbest: 0.5635683 (0)\ttotal: 22.2ms\tremaining: 11m 4s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1144334504\n",
      "bestIteration = 110\n",
      "\n",
      "Shrink model to first 111 iterations.\n",
      "Accuracy Score: 0.95\n",
      "########### Fold number 3 \n",
      "0:\tlearn: 0.5490407\ttest: 0.5766901\tbest: 0.5766901 (0)\ttotal: 21.4ms\tremaining: 10m 41s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.2016859296\n",
      "bestIteration = 42\n",
      "\n",
      "Shrink model to first 43 iterations.\n",
      "Accuracy Score: 0.95\n",
      "########### Fold number 4 \n",
      "0:\tlearn: 0.5439856\ttest: 0.5726367\tbest: 0.5726367 (0)\ttotal: 39.1ms\tremaining: 19m 34s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1245134507\n",
      "bestIteration = 119\n",
      "\n",
      "Shrink model to first 120 iterations.\n",
      "Accuracy Score: 0.94\n",
      "########### Fold number 5 \n",
      "0:\tlearn: 0.5579202\ttest: 0.5815328\tbest: 0.5815328 (0)\ttotal: 23.6ms\tremaining: 11m 46s\n",
      "Stopped by overfitting detector  (300 iterations wait)\n",
      "\n",
      "bestTest = 0.1748149028\n",
      "bestIteration = 35\n",
      "\n",
      "Shrink model to first 36 iterations.\n",
      "Accuracy Score: 0.95\n",
      "Mean accuracy: 0.952\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=SEED) # for cross validation\n",
    "irn_catscores = []\n",
    "irn_catpreds= []\n",
    "\n",
    "\n",
    "# Creating loop for the stratified k fold\n",
    "i = 0\n",
    "for train, val in skf.split(X, y):\n",
    "    print(f'########### Fold number {i+1} ')\n",
    "\n",
    "    # spliting the data\n",
    "    x_train, x_val = X.iloc[train], X.iloc[val]\n",
    "    y_train, y_val = y.iloc[train], y.iloc[val]\n",
    "\n",
    "    clf = CatBoostClassifier(iterations=30000,  has_time=True ,bootstrap_type='No',random_strength=0,\n",
    "                                   learning_rate=0.08,use_best_model=True,\n",
    "                                   random_seed=SEED)\n",
    "    # fitting on train data\n",
    "    clf.fit( x_train, y_train, eval_set = (x_val,y_val),verbose=500 ,early_stopping_rounds=300)\n",
    "\n",
    "    # Making predictions\n",
    "    y_pred = clf.predict(x_val)\n",
    "\n",
    "    # Measuring the accuracy of the model\n",
    "    score = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy Score: {score}')\n",
    "    irn_catscores.append(score)\n",
    "\n",
    "    preds = clf.predict_proba(test_df)\n",
    "    irn_catpreds.append(preds)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "print(f'Mean accuracy: {np.mean(irn_catscores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "zDT2IcewXj5e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "irn_catpreds_mean = np.mean(irn_catpreds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "MhBv4lc8Xj5f",
    "outputId": "eeb14c9c-7df0-41c4-c689-10e9fb8ea7a3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_LNN7BFCVEZKA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_ZMB4I2ZXYE4X</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_OFRXD08BLP3X</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_IQ4IS9AL13PV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_X7ZL15DE59SA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  Target\n",
       "0  ID_LNN7BFCVEZKA       0\n",
       "1  ID_ZMB4I2ZXYE4X       1\n",
       "2  ID_OFRXD08BLP3X       0\n",
       "3  ID_IQ4IS9AL13PV       1\n",
       "4  ID_X7ZL15DE59SA       0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blend_irn = 0.60*irn_lgbmpreds_mean + 0.40*irn_catpreds_mean\n",
    "\n",
    "\n",
    "blend_irn = np.argmax(blend_irn, axis=1)\n",
    "blend_irn = pd.DataFrame({'ID': te_irn.ID, 'Target': blend_irn})\n",
    "blend_irn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_DGmHnlXj5f"
   },
   "source": [
    "### Merge the regional prediction into 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "O-YVfOFCXj5f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_file = pd.concat([blend_irn, blend_afg, blend_sud]) .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "JdPURu12Xj5f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_submission = sample_submission.drop(columns='Target')\n",
    "sample_submission = pd.merge(sample_submission, sub_file, on=['ID'], how='inner')\n",
    "sample_submission.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
